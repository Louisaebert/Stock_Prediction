{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/LouisaEbert/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2021-07-09 18:58:14.650 WARNING root: \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_ticker()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_ticker at 0x7ff2766b2f70>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/en/stable/caching.html#the-hash-funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Hash the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"%s:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    386\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# script path in ScriptRunner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mmain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b8e3defaa95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#fetch all tickers and get index of TSLA as starting point for US market\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_spinner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36mget_or_create_cached_value\u001b[0;34m()\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;31m# If we generated the key earlier we would only hash those\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# globals by name, and miss changes in their code or value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_hash_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# First, get the cache that's attached to this function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36m_hash_func\u001b[0;34m(func, hash_funcs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;31m# because this step will be hashing any objects referenced in the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;31m# body.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     update_hash(\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_hasher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mupdate_hash\u001b[0;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CodeHasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, hasher, obj, context)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInternalHashError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Hash the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"%s:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;31m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"md5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__defaults__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# This works because we set __main__.__file__ to the report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# script path in ScriptRunner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mmain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_ticker()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_ticker at 0x7ff2766b2f70>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/en/stable/caching.html#the-hash-funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import yfinance as yf\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finnhub\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sia\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\n",
    "from ta import add_all_ta_features \n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#cache the stock tickers so they don't have to load everytime\n",
    "@st.cache\n",
    "def get_ticker(market = \"US\"):\n",
    "    tickers = []\n",
    "    t = finnhub_client.stock_symbols(market)\n",
    "    for ticker in t:\n",
    "        tickers.append(ticker[\"displaySymbol\"])\n",
    "    \n",
    "    return tickers\n",
    "\n",
    "#set up finnhub client\n",
    "finnhub_client = finnhub.Client(api_key=\"c2si65iad3ic1qis06lg\")\n",
    "\n",
    "\n",
    "#streamlit interactibles and headings\n",
    "st.header('Using ML to predict the Stock Market')\n",
    "st.subheader('A Big Data Project by Valeriia, Louisa and Alexander')\n",
    "show_news = st.sidebar.checkbox('Display company news?')\n",
    "timeframe = st.sidebar.selectbox('Select period to display historical data:',['max','1d','5d','1mo','3mo','6mo','1y','2y','5y','10y','ytd'])\n",
    "predictframe = st.sidebar.slider('Select days to predict:',min_value=1, max_value = 7, value=7)\n",
    "go = st.sidebar.button(\"Predict!\")\n",
    "\n",
    "\n",
    "#With this part, the user can choose the desired stock market he wants to view, however, US market works best because it has the most data from finnhub so we're limiting it to US\n",
    "#markets = [\"AS\",\"AT\",\"AX\",\"BA\",\"BC\",\"BD\",\"BE\",\"BK\",\"BO\",\"BR\",\"CN\",\"CO\",\"CR\",\"DB\",\"DE\",\"DU\",\"F\",\"HE\",\"HK\",\"HM\",\"IC\",\"IR\",\"IS\",\"JK\",\"JO\",\"KL\",\"KQ\",\"KS\",\"L\",\"LN\",\"LS\",\"MC\",\"ME\",\"MI\",\"MU\",\"MX\",\"NE\",\"NL\",\"NS\",\"NZ\",\"OL\",\"PA\",\"PM\",\"PR\",\"QA\",\"RG\",\"SA\",\"SG\",\"SI\",\"SN\",\"SR\",\"SS\",\"ST\",\"SW\",\"SZ\",\"T\",\"TA\",\"TL\",\"TO\",\"TW\",\"US\",\"V\",\"VI\",\"VN\",\"VS\",\"WA\",\"HA\",\"SX\",\"TG\",\"SC\"]\n",
    "#market = st.sidebar.selectbox('Select market:', markets, index = markets.index(\"US\"))\n",
    "\n",
    "#fetch all tickers and get index of TSLA as starting point for US market\n",
    "ticks = get_ticker()\n",
    "ticks = list(np.unique(ticks))\n",
    "\n",
    "#try except is in case there would be a different market selected\n",
    "try:\n",
    "    index = ticks.index(\"TSLA\")\n",
    "except:\n",
    "    index = 0\n",
    "\n",
    "#Select the desired stock from the market and get stock data from yfinance\n",
    "slct = st.selectbox('Select your stock:',ticks, index = index)\n",
    "\n",
    "#Maybe possible to cache this, however, as stocks are constantly changing, it's kept as-is so that a refresh always shows the newest data \n",
    "def get_stock(slct):\n",
    "    stock = yf.Ticker(slct)\n",
    "    return stock\n",
    "  \n",
    "stock = get_stock(slct)\n",
    "\n",
    "#take historical data from stock in desired timeframe\n",
    "hist = stock.history(period = timeframe)\n",
    "hist['datetime']=hist.index\n",
    "hist.reset_index()\n",
    "hist['datetime']=hist['datetime'].astype('datetime64[ns]')\n",
    "\n",
    "#plot historical data\n",
    "fig = px.line(hist, x=\"datetime\", y=\"Close\", title=f'Closing Price of {slct}', labels = {\"datetime\":\"Date\", \"Close\":\"Closing Price [$]\"},template = 'seaborn')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "with st.spinner(text='Fetching news ...'):\n",
    "    #fetch and add company news\n",
    "    result=(finnhub_client.company_news(slct, _from=\"2020-11-15\", to=\"2021-06-03\"))\n",
    "    news=pd.DataFrame(result)\n",
    "    for i in range(news.shape[0]):\n",
    "        news.datetime[i]= datetime.utcfromtimestamp(news.datetime[i]).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        news['datetime']=news['datetime'].astype('datetime64[ns]')\n",
    "        df=news.merge(hist,on='datetime',how=\"left\")\n",
    "        for i in range(df.shape[0]):\n",
    "            df.summary[i] = strip_numeric(df.summary[i])\n",
    "            df.summary[i] = strip_punctuation(df.summary[i])\n",
    "            df.summary[i] = strip_multiple_whitespaces(df.summary[i])\n",
    "            df.summary[i] = df.summary[i].lower()\n",
    "\n",
    "        for i in range(df.shape[0]):\n",
    "            df.headline[i] = strip_numeric(df.headline[i])\n",
    "            df.headline[i] = strip_punctuation(df.headline[i])\n",
    "            df.headline[i] = strip_multiple_whitespaces(df.headline[i])\n",
    "            df.headline[i] = df.headline[i].lower()\n",
    "\n",
    "        sid = sia()\n",
    "        df['sentiment_vd_headline'] = df['headline'].apply(lambda headline: sid.polarity_scores(headline)['compound'])\n",
    "        df['sentiment_vd_summary'] = df['summary'].apply(lambda summary: sid.polarity_scores(summary)['compound'])\n",
    "        st.write(\"Sentiment analysis of headlines:\", df.sentiment_vd_headline.mean())\n",
    "        st.write(\"Sentiment analysis of summary:\", df.sentiment_vd_summary.mean())\n",
    "\n",
    "    except:\n",
    "        st.write(\"No sentiment analysis possible\")\n",
    "\n",
    "#show additional info if checkbox in sidebar is checked\n",
    "if show_news == True:\n",
    "    try:\n",
    "        st.write(news.headline)\n",
    "    except:\n",
    "        st.write(\"No news available. Look for a more interesting stock.\")\n",
    "\n",
    "\n",
    "# building the data for prediction\n",
    "def get_news(company, date_from='2021-06-01', date_to=None):\n",
    "    '''\n",
    "    returns dataframe with average sentiment of news headline and sentiment of news summary for every date in a given timeframe\n",
    "    company: symbol, example ZM\n",
    "    date_from: string format yyyy-mm-dd\n",
    "    date_to: string format yyyy-mm-dd\n",
    "    '''\n",
    "    sid = sia()\n",
    "    if date_to is None:\n",
    "        date_to = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    URL = 'https://finnhub.io/api/v1/company-news?symbol={}&from={}&to={}&token=c2si65iad3ic1qis06lg'.format(company, date_from, date_to)\n",
    "    r = requests.get(URL)\n",
    "    news_df = pd.DataFrame(r.json())\n",
    "    news_df['datetime'] = [datetime.utcfromtimestamp(i).strftime('%Y-%m-%d') for i in news_df.datetime]\n",
    "    news_df.drop(['id','image', 'related','source', 'url'], axis=1, inplace=True)\n",
    "    news_df['headline_sentiment'] = [sid.polarity_scores(c)['compound'] for c in news_df['headline']]\n",
    "    news_df['summary_sentiment'] = [sid.polarity_scores(c)['compound'] for c in news_df['summary']]\n",
    "    \n",
    "    news_dates = news_df.groupby(['datetime']).mean().sort_index().reset_index()\n",
    "\n",
    "    news_dates['Date'] = pd.to_datetime(news_dates['datetime'], format='%Y-%m-%d')\n",
    "    return news_dates\n",
    "\n",
    "\n",
    "def get_recommendation_trends(company):\n",
    "    URL = 'https://finnhub.io/api/v1/stock/recommendation?symbol={}&token=c2si65iad3ic1qis06lg'.format(company)\n",
    "    r = requests.get(URL)\n",
    "    recommendation_df = pd.DataFrame(r.json())\n",
    "    dfs = []\n",
    "    for i, month in recommendation_df.period.iteritems():\n",
    "        period = month\n",
    "        df_month = pd.DataFrame({\n",
    "        'period': pd.date_range(\n",
    "            start = pd.Timestamp(period),                        \n",
    "            end = pd.Timestamp(period) + pd.offsets.MonthEnd(0),  # <-- 2018-08-31 with MonthEnd\n",
    "            freq = 'D'\n",
    "            )\n",
    "        })\n",
    "        df_month['month'] = month\n",
    "        dfs.append(df_month)\n",
    "    df = pd.concat(dfs)\n",
    "    df.period = [datetime.strftime(x, '%Y-%m-%d') for x in df.period]\n",
    "    df = df.merge(recommendation_df,how='left', left_on='month', right_on='period')\n",
    "    df.drop(['month','period_y'], axis=1, inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['period_x'], format='%Y-%m-%d')\n",
    "    return df.drop(['period_x'],1)\n",
    "\n",
    "with st.spinner(text='Fetching additional data ...'):\n",
    "    add_all_ta_features(\n",
    "        hist, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "\n",
    "    news_df = get_news(slct)\n",
    "    trend_df = get_recommendation_trends(slct)\n",
    "    data = hist.merge(news_df, how='inner', on='Date').merge(trend_df, how='inner', on='Date').drop(['symbol','datetime_y','datetime_x'],1)\n",
    "    st.write('Data to train the model on')\n",
    "    st.write(data)\n",
    "\n",
    "def predict(data, move_days=7):\n",
    "    # shifted values column, like this the model can learn what the price is going to be x days later\n",
    "    data[\"shift_close\"]=data[[\"Close\"]].shift(-move_days)\n",
    "    #x without the NaNs \n",
    "    X = data.drop(['shift_close','Date'],1)[:-move_days]\n",
    "    y = data[['shift_close']][:-move_days]\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "    # training a model on the dataset where we have all data\n",
    "    param_grid = {'min_samples_leaf': [2,3,5,10,20], 'n_estimators': [3,5,10,50,100], 'max_depth':[20,30,40]}\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    params = grid_search.best_params_\n",
    "\n",
    "    rf=RandomForestRegressor(min_samples_leaf=params['min_samples_leaf'], n_estimators=params['n_estimators'], max_depth = params['max_depth'],random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_existing=rf.predict(X_test)\n",
    "    MSE = mean_squared_error(y_test, y_pred_existing)\n",
    "\n",
    "    # View accuracy score\n",
    "    score = rf.score(X_test, y_pred_existing)\n",
    "\n",
    "    # Now the X variable is going to be the features for the days where we have no \"in x Days price\"\n",
    "    last_days=data.tail(move_days)\n",
    "    X_pred=last_days.drop(['shift_close','Date'],1)\n",
    "    \n",
    "    #values for the next 7 days \n",
    "    pred=rf.predict(X_pred)\n",
    "    #predicted_dates = data['Date']\n",
    "    last_days['shift_close'] = pred\n",
    "    \n",
    "    return last_days, MSE, score\n",
    "\n",
    "if go == True:\n",
    "    with st.spinner(text='Calculating prediction ...'):\n",
    "        predicted, MSE, score =predict(data, move_days=predictframe)\n",
    "\n",
    "        #setting the index to date\n",
    "        predicted.index = predicted.Date\n",
    "\n",
    "        #shifting it back forward for x days\n",
    "        predicted.index = predicted.index.shift(predictframe, freq = \"D\")\n",
    "        st.write('Predicted values')\n",
    "        st.write(predicted[['Date','shift_close']])\n",
    "\n",
    "\n",
    "        st.write('MSE:' + str(MSE))\n",
    "        st.write('Score:' + str(score))\n",
    "\n",
    "        #joining with the historicaL data to create a subframe with just the new dates then appending that to the rest of the data but only take last 20 entries, sorted by date\n",
    "        graph = hist.append(hist.join(predicted[[\"Date\", \"shift_close\"]], how= 'right')).sort_index().tail(20)\n",
    "        graph.rename(columns = {\"shift_close\":\"Prediction\", \"Close\":\"Historical Data\"}, inplace = True)\n",
    "\n",
    "        #plotting\n",
    "        fig2 = px.line(graph, x=graph.index, y=[\"Historical Data\",\"Prediction\"], title=f'Predicted closing price for {slct}', labels = {\"index\":\"Date\"}, template = 'seaborn', range_x = [min(graph.index),max(graph.index.shift(1, freq = \"D\"))])\n",
    "        fig2.update_traces(mode='markers+lines')\n",
    "        fig2.update_layout(xaxis_title='Date', yaxis_title='Closing Price [$]')\n",
    "        st.plotly_chart(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
